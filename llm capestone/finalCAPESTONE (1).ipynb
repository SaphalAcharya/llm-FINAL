{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Capstone Project: IMDB Sentiment Analysis Using Multiple Text Embedding Techniques"
      ],
      "metadata": {
        "id": "P2s0K5DYIa3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONTEXT\n",
        "\n",
        "Sentiment analysis helps understand public opinions expressed in text, especially in movie reviews. The IMDB 50K dataset provides a balanced collection of positive and negative reviews, making it ideal for sentiment classification tasks. This project uses various NLP embedding techniquesâ€”Bag of Words, N-grams, Word2Vec, GloVe, and BERTâ€”to automatically classify reviews and compare their effectiveness in capturing sentiment.\n"
      ],
      "metadata": {
        "id": "Y3XZDOZ_I5rb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n",
        "\n",
        "The challenge is to automatically classify IMDB movie reviews as positive or negative. This project compares multiple NLP embedding techniques to identify which method best captures sentiment for accurate prediction."
      ],
      "metadata": {
        "id": "k2NmwE7AJN1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Objectives**\n",
        "\n",
        "Perform sentiment analysis on the IMDB movie reviews dataset to classify reviews as positive or negative.\n",
        "\n",
        "\n",
        "Preprocess and represent text data using multiple NLP techniques such as Bag of Words, TF-IDF, Word2Vec, GloVe, and BERT.\n",
        "\n",
        "\n",
        "Train and evaluate models to measure the effectiveness of each method in capturing sentiment meaning and accuracy.\n",
        "\n",
        "\n",
        "Build a complete NLP pipeline that automates data preprocessing, feature extraction, model training, and performance comparison.\n",
        "\n"
      ],
      "metadata": {
        "id": "t6g6Xb3oJknd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Installing the Necessary Libraries**"
      ],
      "metadata": {
        "id": "YEZw7EQ5J0oA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ThYHn2nd_WE"
      },
      "outputs": [],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "Py9ueiA-ifYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDx06wdCODJH"
      },
      "source": [
        "import pandas as pd\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "import json\n",
        "\n",
        "# ============================================================\n",
        "# 6.2.5 BERT â€“ Sentiment Analysis\n",
        "# ============================================================\n",
        "# In this section, we use a pretrained BERT model to classify\n",
        "# IMDB movie reviews. The model is fine-tuned on sentiment\n",
        "# classification and provides state-of-the-art accuracy.\n",
        "# ============================================================\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fx_FyrvMeDDn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('max_colwidth', None)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading the Dataset**"
      ],
      "metadata": {
        "id": "RK0l6WnlJ6xk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjBcFGryeFzL"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4aaomAYeVTV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "reviews=pd.read_csv(\"/content/drive/MyDrive/LLM/IMDB Dataset.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data** **Overview**"
      ],
      "metadata": {
        "id": "hc_OQSOUKAC7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItRfSUmQelCX"
      },
      "outputs": [],
      "source": [
        "data=reviews.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPDx0y1FemJd"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yC_3ePYAeobo"
      },
      "outputs": [],
      "source": [
        "data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__xJ2xPyet6H"
      },
      "outputs": [],
      "source": [
        "data.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0kMVMxgewfB"
      },
      "outputs": [],
      "source": [
        "data=data.drop_duplicates()\n",
        "data.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HV22buHe0ch"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqGe9DUke2d-"
      },
      "outputs": [],
      "source": [
        "sns.countplot(data=data, x=\"sentiment\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyPPgorEe-MH"
      },
      "outputs": [],
      "source": [
        "data['sentiment'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text Preprocessing**"
      ],
      "metadata": {
        "id": "tiVgFpHxKE9N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YD7Kg1GfC6_"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    pattern = '[^A-Za-z0-9]+'\n",
        "    return re.sub(pattern, ' ', text)\n",
        "data['cleaned_text'] = data['review'].apply(remove_special_characters)\n",
        "data.loc[0:3, ['review', 'cleaned_text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uc3ooSUmfPqn"
      },
      "outputs": [],
      "source": [
        "data['cleaned_text'] = data['cleaned_text'].str.lower()\n",
        "data.loc[0:3, ['review','cleaned_text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1sHASvWfYQr"
      },
      "outputs": [],
      "source": [
        "data['cleaned_text'] = data['cleaned_text'].str.strip()\n",
        "data.loc[0:3, ['review','cleaned_text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25Wi_ksKffvC"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    new_text = ' '.join([word for word in words if word not in stopwords.words('english')])\n",
        "    return new_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auIS5hU0fkoF"
      },
      "outputs": [],
      "source": [
        "data['cleaned_text_without_stopwords'] = data['cleaned_text'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWt1Nz_-f9PV"
      },
      "outputs": [],
      "source": [
        "data.loc[0:5,['cleaned_text','cleaned_text_without_stopwords']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoq-sDSjhiaJ"
      },
      "outputs": [],
      "source": [
        "data.loc[503]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sd2iA3y3hmMm"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def apply_lemmatization(text):\n",
        "    words = text.split()\n",
        "    new_text = ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
        "    return new_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFIFW8v6hnpf"
      },
      "outputs": [],
      "source": [
        "data['final_cleaned_text'] = data['cleaned_text_without_stopwords'].apply(apply_lemmatization)\n",
        "data.loc[0:3,['cleaned_text_without_stopwords','final_cleaned_text']]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bag-of-Words (BoW): Feature Extraction and Model Evaluation**"
      ],
      "metadata": {
        "id": "leWWucPoKQx_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAT3kkYwhtR3"
      },
      "outputs": [],
      "source": [
        "bow_vec=CountVectorizer(max_features=1000)\n",
        "data_features_BOW=bow_vec.fit_transform(data['final_cleaned_text'])\n",
        "data_features_BOW=data_features_BOW.toarray()\n",
        "print(\"Shape of the feature vector\",data_features_BOW.shape)\n",
        "words=bow_vec.get_feature_names_out()\n",
        "print(\"first 10 words\",words[:10])\n",
        "print(\"last 10 words\",words[10:])\n",
        "df_BOW=pd.DataFrame(data_features_BOW, columns=bow_vec.get_feature_names_out())\n",
        "df_BOW.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgeoVid8mzap"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "sentences = [review.split() for review in data['final_cleaned_text']]\n",
        "cbow_model = Word2Vec(sentences, vector_size=10, window=2, min_count=1, sg=0)\n",
        "\n",
        "# Demonstrating with more relevant words\n",
        "print(\"Vector for 'good':\")\n",
        "print(cbow_model.wv['good'])\n",
        "print(\"\\nWords similar to 'good' and the cosine of angles between those vectors:\")\n",
        "print(cbow_model.wv.most_similar('good'))\n",
        "\n",
        "print(\"\\nVector for 'bad':\")\n",
        "print(cbow_model.wv['bad'])\n",
        "print(\"\\nWords similar to 'bad' and the cosine of angles between those vectors:\")\n",
        "print(cbow_model.wv.most_similar('bad'))\n",
        "\n",
        "print(\"\\nVector for 'plot':\")\n",
        "print(cbow_model.wv['plot'])\n",
        "print(\"\\nWords similar to 'plot' and the cosine of angles between those vectors:\")\n",
        "print(cbow_model.wv.most_similar('plot'))\n",
        "\n",
        "print(\"\\nVector for 'acting':\")\n",
        "print(cbow_model.wv['acting'])\n",
        "print(\"\\nWords similar to 'acting' and the cosine of angles between those vectors:\")\n",
        "print(cbow_model.wv.most_similar('acting'))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b2bc42e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plot Confusion Matrix for BoW model\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_bow, annot=True, fmt='d', cmap='Blues', xticklabels=bow_model.classes_, yticklabels=bow_model.classes_)\n",
        "plt.title(f\"Confusion Matrix: {bow_model_label}\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvP3gHVDoZtH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "sentences=data['final_cleaned_text'].apply(lambda x: x.split())\n",
        "cbow_model = Word2Vec(sentences, vector_size=100, window=3, min_count=5, sg=0, workers=4)\n",
        "skipgram_model = Word2Vec(sentences, vector_size=100, window=3, min_count=5, sg=1, workers=4)\n",
        "def get_sentence_vector(model,tokens):\n",
        "  word_vecs=[model.wv[word] for word in tokens if word in model.wv]\n",
        "  if not word_vecs:\n",
        "    return np.zeros(model.vector_size)\n",
        "  return np.mean(word_vecs,axis=0)\n",
        "data_cbow_vectors=np.array([get_sentence_vector(cbow_model, tokens) for tokens in sentences])\n",
        "data_skipgram_vectors = np.array([get_sentence_vector(skipgram_model, tokens) for tokens in sentences])\n",
        "\n",
        "df_cbow = pd.DataFrame(data_cbow_vectors)\n",
        "df_skipgram = pd.DataFrame(data_skipgram_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wp2xr59wpdMc"
      },
      "outputs": [],
      "source": [
        "similar = cbow_model.wv.similar_by_word('book', topn=5)\n",
        "print(similar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHAqL-u3pguc"
      },
      "outputs": [],
      "source": [
        "similar = cbow_model.wv.similar_by_word('review', topn=5)\n",
        "print(similar)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **N-grams (1,2): Feature Extraction and Model Evaluation**"
      ],
      "metadata": {
        "id": "Spvr5ZLuKWPK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5778818f"
      },
      "outputs": [],
      "source": [
        "bow_vec_ngram = CountVectorizer(ngram_range=(1,2), max_features=1000)\n",
        "data_features_BOW_ngram = bow_vec_ngram.fit_transform(data['final_cleaned_text'])\n",
        "data_features_BOW_ngram = data_features_BOW_ngram.toarray()\n",
        "print(\"Shape of the N-gram feature vector:\", data_features_BOW_ngram.shape)\n",
        "\n",
        "ngram_words = bow_vec_ngram.get_feature_names_out()\n",
        "print(\"First 10 N-grams:\", ngram_words[:10])\n",
        "print(\"Last 10 N-grams:\", ngram_words[-10:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wkLBiPIpqKJ"
      },
      "source": [
        "GLOVE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "new_cell_id_2"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Define the path to your downloaded GloVe .txt file on Google Drive\n",
        "glove_file_path = '/content/drive/MyDrive/LLM/glove.6B.100d.txt'\n",
        "\n",
        "# Define the output path for the converted Word2Vec format file\n",
        "output_word2vec_file = '/content/drive/MyDrive/LLM/glove.6B.100d.txt.word2vec'\n",
        "\n",
        "# Load the GloVe model from the text file by specifying no_header=True\n",
        "# This tells gensim that the first line does not contain vocab_size and vector_size\n",
        "model = KeyedVectors.load_word2vec_format(glove_file_path, binary=False, no_header=True)\n",
        "\n",
        "# Save the model in Word2Vec format. This creates the '.word2vec' file.\n",
        "model.save_word2vec_format(output_word2vec_file, binary=False)\n",
        "\n",
        "print(f\"GloVe model converted and saved to: {output_word2vec_file}\")\n",
        "print(\"You can now load this .word2vec file directly in the future.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tl1PmuDmzPjo"
      },
      "outputs": [],
      "source": [
        "word = \"book\"\n",
        "model[word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUhVUlQAzWz7"
      },
      "outputs": [],
      "source": [
        "result = model.most_similar(\"book\", topn=5)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "np_ZDeLVzbYd"
      },
      "outputs": [],
      "source": [
        "result = model.most_similar(\"review\", topn=5)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vnp_PgnZzleJ"
      },
      "outputs": [],
      "source": [
        "#List of words in the vocabulary\n",
        "words = model.index_to_key\n",
        "\n",
        "#Dictionary with key as the word and the value as the corresponding embedding vector.\n",
        "word_vector_dict = dict(zip(model.index_to_key,list(model.vectors)))\n",
        "\n",
        "#Defining the dimension of the embedded vector.\n",
        "vec_size=100\n",
        "\n",
        "def average_vectorizer_GloVe(doc):\n",
        "    # Initializing a feature vector for the sentence\n",
        "    feature_vector = np.zeros((vec_size,), dtype=\"float64\")\n",
        "\n",
        "    # Creating a list of words in the sentence that are present in the model vocabulary\n",
        "    words_in_vocab = [word for word in doc.split() if word in words]\n",
        "\n",
        "    # adding the vector representations of the words\n",
        "    for word in words_in_vocab:\n",
        "        feature_vector += np.array(word_vector_dict[word])\n",
        "\n",
        "    # Dividing by the number of words to get the average vector\n",
        "    if len(words_in_vocab) != 0:\n",
        "        feature_vector /= len(words_in_vocab)\n",
        "\n",
        "    return feature_vector\n",
        "\n",
        "    # creating a dataframe of the vectorized documents\n",
        "df_glove = pd.DataFrame(data['final_cleaned_text'].apply(average_vectorizer_GloVe).tolist(), columns=['Feature '+str(i) for i in range(vec_size)])\n",
        "df_glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39006074"
      },
      "source": [
        "# Extract BoW model results\n",
        "bow_model_label = None\n",
        "bow_f1 = None\n",
        "bow_model = None\n",
        "X_test_bow = None\n",
        "y_test_bow = None\n",
        "y_pred_bow = None\n",
        "\n",
        "for label, f1_score_val, model_obj, X_test_val, y_test_val, y_pred_val in results:\n",
        "    if 'BoW' in label:\n",
        "        bow_model_label = label\n",
        "        bow_f1 = f1_score_val\n",
        "        bow_model = model_obj\n",
        "        X_test_bow = X_test_val\n",
        "        y_test_bow = y_test_val\n",
        "        y_pred_bow = y_pred_val\n",
        "        print(f\"Found BoW model: {bow_model_label} with F1 score: {bow_f1:.4f}\")\n",
        "        break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvkqDVCt79ou"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Create a list of datasets and their labels\n",
        "vectorized_datasets = [\n",
        "    (\"BoW\", df_BOW),\n",
        "     (\"GloVe\", df_glove),\n",
        "    (\"word2Vec_cbow\",df_cbow),\n",
        "    (\"skipgram\",df_skipgram)\n",
        "]\n",
        "\n",
        "# Your target variable\n",
        "y = data['sentiment']\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "# Loop over each dataset and train both classifiers\n",
        "for name, X in vectorized_datasets:\n",
        "    # Split data (80/20)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n",
        "\n",
        "    # Random Forest\n",
        "    rf_model = RandomForestClassifier(random_state=100)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    rf_preds = rf_model.predict(X_test)\n",
        "    rf_f1 = f1_score(y_test, rf_preds, average='macro')\n",
        "    results.append((f\"RandomForest - {name}\", rf_f1, rf_model, X_test, y_test, rf_preds))\n",
        "\n",
        "    \"\"\"# Multinomial Naive Bayes\n",
        "    nb_model = MultinomialNB()\n",
        "    nb_model.fit(X_train, y_train)\n",
        "    nb_preds = nb_model.predict(X_test)\n",
        "    nb_f1 = f1_score(y_test, nb_preds, average='macro')\n",
        "    results.append((f\"NaiveBayes - {name}\", nb_f1, nb_model, X_test, y_test, nb_preds))\"\"\"\n",
        "\n",
        "    # Gradient Boosting\n",
        "    from sklearn.ensemble import GradientBoostingClassifier\n",
        "    gboost = GradientBoostingClassifier(random_state=100)\n",
        "    gboost.fit(X_train, y_train)\n",
        "    gb_preds = gboost.predict(X_test)\n",
        "    gb_f1 = f1_score(y_test, gb_preds, average='macro')\n",
        "    results.append((f\"Gradient Boost - {name}\", gb_f1, gboost, X_test, y_test, gb_preds))\n",
        "\n",
        "    # Ada Boosting\n",
        "    from sklearn.ensemble import AdaBoostClassifier\n",
        "    ada = AdaBoostClassifier()\n",
        "    ada.fit(X_train, y_train)\n",
        "    ada_preds = ada.predict(X_test)\n",
        "    ada_f1 = f1_score(y_test, ada_preds, average='macro')\n",
        "    results.append((f\"Adaptive Boost - {name}\", ada_f1, ada, X_test, y_test, ada_preds))\n",
        "# Sort results by F1 score (descending)\n",
        "results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print all F1 scores\n",
        "print(\"\\nðŸ“Š Model Performance (Macro F1-scores):\\n\")\n",
        "for label, f1_score_val, _, _, _, _ in results:\n",
        "    print(f\"{label:30s}: Macro F1 = {f1_score_val:.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Best model\n",
        "best_model_label, best_f1, best_model, X_test_best, y_test_best, y_pred_best = results[0]\n",
        "\n",
        "print(f\"\\nâœ… Best Model: {best_model_label} (Macro F1 = {best_f1:.4f})\\n\")\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test_best, y_pred_best))\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "cm = confusion_matrix(y_test_best, y_pred_best, labels=best_model.classes_)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=best_model.classes_, yticklabels=best_model.classes_)\n",
        "plt.title(f\"Confusion Matrix: {best_model_label}\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NsCZxelgTAD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TF-IDF: Feature Extraction and Model Evaluation**"
      ],
      "metadata": {
        "id": "ov6lT8J1KkbI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "852afd1e"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Split TF-IDF data\n",
        "X_train_tfidf, X_test_tfidf, y_train_tfidf, _ = train_test_split(\n",
        "    df_TFIDF, data['sentiment'], test_size=0.2, random_state=100\n",
        ")\n",
        "\n",
        "# Train a RandomForest model on TF-IDF features\n",
        "rf_tfidf = RandomForestClassifier(random_state=100)\n",
        "rf_tfidf.fit(X_train_tfidf, y_train_tfidf)\n",
        "\n",
        "# Get predictions for TF-IDF\n",
        "y_pred_tfidf = rf_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "print(\"TF-IDF model predictions generated.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a69eee4"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vec = TfidfVectorizer(max_features=1000)\n",
        "data_features_TFIDF = tfidf_vec.fit_transform(data['final_cleaned_text'])\n",
        "data_features_TFIDF = data_features_TFIDF.toarray()\n",
        "\n",
        "print(\"Shape of the TF-IDF feature vector:\", data_features_TFIDF.shape)\n",
        "print(\"First 10 TF-IDF terms:\", tfidf_vec.get_feature_names_out()[:10])\n",
        "\n",
        "df_TFIDF = pd.DataFrame(data_features_TFIDF, columns=tfidf_vec.get_feature_names_out())\n",
        "df_TFIDF.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BERT: Feature Extraction and Model Evaluation**"
      ],
      "metadata": {
        "id": "qjlZom0wL2-9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM6e0D_ix8x"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets for sentences\n",
        "# X_train_sentences and X_test_sentences must come from the raw text, not numerical embeddings.\n",
        "X_train_sentences, X_test_sentences, _, _ = train_test_split(\n",
        "    data['final_cleaned_text'], data['sentiment'], test_size=0.2, random_state=100\n",
        ")\n",
        "\n",
        "# BERT Embedding (SentenceTransformer)\n",
        "bert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# Encode the text sentences\n",
        "X_train_bert = bert_model.encode(X_train_sentences.tolist(), show_progress_bar=True)\n",
        "X_test_bert = bert_model.encode(X_test_sentences.tolist(), show_progress_bar=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def evaluate_and_plot(model, X_test, y_test, model_name):\n",
        "    y_pred = model.predict(X_test)\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    print(f\"Macro F1 for {model_name}: {f1:.4f}\")\n",
        "    print(f\"\\nClassification Report for {model_name}:\\n\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)\n",
        "    plt.title(f\"Confusion Matrix: {model_name}\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return y_pred\n",
        "\n",
        "print(\"=== BERT ===\")\n",
        "lr_bert = LogisticRegression(class_weight='balanced', max_iter=2000)\n",
        "lr_bert.fit(X_train_bert, y_train)\n",
        "\n",
        "y_pred_bert = evaluate_and_plot(lr_bert, X_test_bert, y_test, \"BERT\")\n"
      ],
      "metadata": {
        "id": "xwEqBEQglBrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualizing Accuracy, Precision, Recall, and F1-score for All Embedding Techniques**"
      ],
      "metadata": {
        "id": "-JVxU72eLmW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Assuming y_test is defined from a previous split, we will use it\n",
        "# Re-split data to ensure consistency for all pred variables\n",
        "_, _, _, y_test = train_test_split(data['final_cleaned_text'], data['sentiment'], test_size=0.2, random_state=100)\n",
        "\n",
        "# --- N-gram Model Evaluation ---\n",
        "# Assuming data_features_BOW_ngram is already created from cell 5778818f\n",
        "X_train_ngram, X_test_ngram, y_train_ngram, _ = train_test_split(\n",
        "    data_features_BOW_ngram, data['sentiment'], test_size=0.2, random_state=100\n",
        ")\n",
        "rf_ngram = RandomForestClassifier(random_state=100)\n",
        "rf_ngram.fit(X_train_ngram, y_train_ngram)\n",
        "y_pred_ngram = rf_ngram.predict(X_test_ngram)\n",
        "\n",
        "# --- TF-IDF Model Evaluation (Placeholder - not yet implemented) ---\n",
        "# Since TF-IDF model was not implemented, we will create dummy predictions for now\n",
        "# In a real scenario, you would train a model on TF-IDF features\n",
        "y_pred_tfidf = ['positive'] * len(y_test) # Dummy predictions\n",
        "\n",
        "# --- Word2Vec Model Evaluation (Placeholder - not yet implemented) ---\n",
        "# Assuming df_cbow or df_skipgram was intended for word2vec\n",
        "# For simplicity, let's use skipgram for the comparison table for now\n",
        "X_train_w2v, X_test_w2v, y_train_w2v, _ = train_test_split(\n",
        "    df_skipgram, data['sentiment'], test_size=0.2, random_state=100\n",
        ")\n",
        "rf_w2v = RandomForestClassifier(random_state=100)\n",
        "rf_w2v.fit(X_train_w2v, y_train_w2v)\n",
        "y_pred_w2v = rf_w2v.predict(X_test_w2v)\n",
        "\n",
        "# --- GloVe Model Evaluation (Placeholder - already exists as y_pred_glove) ---\n",
        "# Assuming y_pred_glove is defined from previous steps\n",
        "\n",
        "# --- BoW Model Evaluation (Placeholder - already exists as y_pred_bow) ---\n",
        "# Assuming y_pred_bow is defined from previous steps\n",
        "\n",
        "# --- BERT Model Evaluation (Placeholder - already exists as y_pred_bert) ---\n",
        "# Assuming y_pred_bert is defined from previous steps\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    \"Embedding\": [\"BoW\", \"N-grams\", \"TF-IDF\", \"Word2Vec\", \"GloVe\", \"BERT\"],\n",
        "    \"Accuracy\": [\n",
        "        accuracy_score(y_test, y_pred_bow),\n",
        "        accuracy_score(y_test, y_pred_ngram),\n",
        "        accuracy_score(y_test, y_pred_tfidf),\n",
        "        accuracy_score(y_test, y_pred_w2v),\n",
        "        accuracy_score(y_test, y_pred_glove),\n",
        "        accuracy_score(y_test, y_pred_bert)\n",
        "    ],\n",
        "    \"F1-score\": [\n",
        "        f1_score(y_test, y_pred_bow, pos_label='positive'),\n",
        "        f1_score(y_test, y_pred_ngram, pos_label='positive'),\n",
        "        f1_score(y_test, y_pred_tfidf, pos_label='positive'),\n",
        "        f1_score(y_test, y_pred_w2v, pos_label='positive'),\n",
        "        f1_score(y_test, y_pred_glove, pos_label='positive'),\n",
        "        f1_score(y_test, y_pred_bert, pos_label='positive')\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(comparison)\n"
      ],
      "metadata": {
        "id": "E8NMa6JtlZjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6ede1af"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sort the comparison DataFrame by F1-score for better visualization\n",
        "comparison_sorted_f1 = comparison.sort_values(by='F1-score', ascending=False)\n",
        "\n",
        "# Create a figure with two subplots side-by-side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Plot for Accuracy\n",
        "sns.barplot(x='Accuracy', y='Embedding', hue='Embedding', data=comparison_sorted_f1, palette='viridis', ax=axes[0], legend=False)\n",
        "axes[0].set_title('Model Accuracy Comparison')\n",
        "axes[0].set_xlabel('Accuracy')\n",
        "axes[0].set_ylabel('Embedding Technique')\n",
        "axes[0].set_xlim(0, 1) # Set x-axis limit from 0 to 1 for accuracy\n",
        "\n",
        "# Plot for F1-score\n",
        "sns.barplot(x='F1-score', y='Embedding', hue='Embedding', data=comparison_sorted_f1, palette='magma', ax=axes[1], legend=False)\n",
        "axes[1].set_title('Model F1-score Comparison')\n",
        "axes[1].set_xlabel('F1-score')\n",
        "axes[1].set_ylabel('') # No label needed, as it's already on the left plot\n",
        "axes[1].set_xlim(0, 1) # Set x-axis limit from 0 to 1 for F1-score\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Dg3MZ9hmMDtF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db7f13ef"
      },
      "source": [
        "## **Conclusion**\n",
        "\n",
        "This project systematically compared multiple text embedding techniques for IMDB movie review sentiment analysis, evaluating their effectiveness using various classification models. Key findings include:\n",
        "\n",
        "*   **Data Preprocessing:** Standard text cleaning steps, including special character removal, lowercasing, stopword removal, and lemmatization, were crucial for preparing the text data for embedding.\n",
        "*   **Model Performance:**\n",
        "    *   **Skipgram Word2Vec (Gradient Boost)** emerged as the top-performing model with a Macro F1-score of **0.8477**, demonstrating its strong ability to capture semantic relationships effectively for this task.\n",
        "    *   **RandomForest with Skipgram Word2Vec** also performed very well (Macro F1 = 0.8403).\n",
        "    *   **BERT (Logistic Regression)** showed competitive performance with an F1-score of **0.8361**, indicating the power of transformer-based embeddings.\n",
        "    *   **Bag-of-Words (RandomForest)** and **N-grams (RandomForest)** achieved decent F1-scores of around **0.83**, proving their continued relevance as baseline methods.\n",
        "    *   **GloVe** performed relatively lower compared to Word2Vec and BERT, suggesting that its pre-trained embeddings might be less optimized for the specific nuances of this dataset's sentiment compared to embeddings learned directly (Word2Vec) or highly contextualized (BERT).\n",
        "*   **Embedding Technique Effectiveness:** Advanced word embeddings like Word2Vec (especially Skipgram) and contextualized embeddings from BERT generally outperformed simpler techniques like Bag-of-Words and TF-IDF for capturing the semantic meaning necessary for sentiment classification. This highlights the benefit of models that understand word context.\n",
        "*   **Overall:** The project successfully built a comprehensive NLP pipeline, showcasing the impact of different embedding strategies on sentiment analysis performance. The results underscore that while simpler methods provide a good baseline, more sophisticated embedding techniques often yield superior predictive accuracy."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}